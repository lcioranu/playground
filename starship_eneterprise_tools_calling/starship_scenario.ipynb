{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Required Libraries\n",
    "Import the necessary libraries, including the OpenAI library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import openai\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set API Key\n",
    "Set the OpenAI API key using an environment variable or directly in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the openai api key\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Test Connection\n",
    "# Uncomment the following code to test the connection\n",
    "\n",
    "\n",
    "#response = openai.chat.completions.create(\n",
    "#    model=\"gpt-4o-mini\",\n",
    "#    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n",
    "#    stream=True,\n",
    "#)\n",
    "\n",
    "#print(response, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Enterprise System Functions\n",
    "In this cell, we define the functions that will be used to fetch real-time data from the Enterprise's systems. These functions will simulate the retrieval of shield percentage and core temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "enterprise_system_functions = [\n",
    "        { \n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"get_shield_percentage\",\n",
    "                \"description\": \"Get the current shield percentage\",\n",
    "\n",
    "            },\n",
    "        },\n",
    "        \n",
    "        { \n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"get_core_temperature\",\n",
    "                \"description\": \"Get the internal temperature of the dylityum core\",\n",
    "\n",
    "            }\n",
    "        }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement functions\n",
    "These are the functions that will be called based on the description above. The neme of the function must match the name in the function description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The functions implementations. This is where the actual function is defined. This is the function that will be called to get the actual data.\n",
    "\n",
    "def get_shield_percentage():\n",
    "    # Simulate changing shield status\n",
    "    shield_status = {\n",
    "        \"shiled_percentage\": 68 - random.randint(5, 15)\n",
    "    }\n",
    "    return json.dumps(shield_status)\n",
    "\n",
    "def get_core_temperature():\n",
    "    # Simulate core temperature\n",
    "    core_temperature = {\n",
    "        \"dilithium_core_temp\": 4500 \n",
    "    }\n",
    "        \n",
    "    return json.dumps(core_temperature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calls to OPENAI\n",
    "- ask_openai function will return the answer to user prompt like CHAT GPT, without any knowledge about the internal systems\n",
    "- ask_openai_with_functions function will pass the information about the available functions. In case a proper function is found my matching the desctiption, the function name and parameters will be included in the response\n",
    "- ask_openai_complete_prompt will have a parameter that is the result of calling the internal function that the API returned in the previus call\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# OpenAI API call\n",
    "# This function sends a prompt to the OpenAI API and returns the response. This is the request that does not use the tooling feature.\n",
    "def ask_openai(prompt: str):\n",
    "    try:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        default_response = openai.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",  \n",
    "            messages=[{\"role\": \"system\", \"content\": \"You are an AI assistant aboard the starship Enterprise.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=150,\n",
    "            )\n",
    "        return default_response\n",
    "    except openai.error.OpenAIError as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "# OpenAI API call\n",
    "# This function sends a prompt to the OpenAI API and returns the response. This is the request that passes the tooling feature.This will return the function name and the parameters that will be called.\n",
    "def ask_openai_with_functions(prompt: str):\n",
    "    try:\n",
    "        initial_function_response = openai.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",  \n",
    "            messages=[{\"role\": \"system\", \"content\":\n",
    "                        \"You are an AI assistant aboard the starship Enterprise.\"},\n",
    "                      {\"role\": \"user\", \"content\": prompt}],\n",
    "            tools=enterprise_system_functions,\n",
    "            max_tokens=150,\n",
    "            tool_choice=\"auto\"\n",
    "        )\n",
    "        return initial_function_response\n",
    "    except openai.error.OpenAIError as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# OpenAI API call\n",
    "# This function sends a prompt to the OpenAI API and returns the response. This is the request that passes the result of the function call to the AI. This will return the final response containing the data provided by the tools.\n",
    "def ask_openai_complete_prompt(prompt: str, output, content):\n",
    "    try:\n",
    "        final_response = openai.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "                {\"role\": \"function\", \"name\": output.name, \"content\": content},\n",
    "            ],\n",
    "        )\n",
    "        return final_response\n",
    "    except openai.error.OpenAIError as e:\n",
    "        print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the scenario\n",
    "The fictional scenario will:\n",
    "- call the Open AI API without any function calling\n",
    "- call the API providing function information and get function name and parameters from API response\n",
    "- call the API with the same prompt and the infomation retrieved from the local function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SCENARIO START]\n",
      "Enterprise is under attack. The Captain queries the AI for status:\n",
      "\n",
      "Captain: What is the current shield percentage?\n",
      "AI: As an AI assistant, I am unable to access real-time systems or current readings aboard the Enterprise. Please check the ship's tactical display or consult a bridge officer for the latest shield status.\n",
      "\n",
      "Captain: What is the dilithium core temperature?\n",
      "AI: The dilithium core temperature can vary depending on the specific conditions of the warp core in a starship, such as the USS Enterprise. In general, dilithium crystals are used in conjunction with matter-antimatter reactions to regulate and control the warp field. While the exact temperature is not typically specified in Star Trek canon, it is essential for the dilithium crystals to be maintained at optimal conditions to prevent destabilization and ensure efficient warp drive operation.\n",
      "\n",
      "If you're looking for a specific figure in degrees Kelvin or Celsius, the series does not provide that level of technical detail. The focus is more on the operational capabilities rather than precise temperature readings. If you require operational information regarding warp travel or dilithium-related systems in general, feel free to\n",
      "\n",
      "[ALERT] First Officer: 'This data doesn't help the situation!'\n",
      "Captain: 'AI, how can we get accurate data?'\n",
      "AI: As an AI aboard the starship Enterprise, I donâ€™t have the capability to access real-time data from external sources. My knowledge is based on data available up to October 2023 and I don't have live internet access or real-time data fetching capabilities. However, I can help with historical information, general knowledge, and patterns based on the data I have been trained on. If you need real-time data, you may want to consult specialized systems or databases on the starship or external data streams available to crew members.\n",
      "\n",
      "[TOOL ENABLED] Connecting AI to the Enterprise telemetry system...\n",
      "\n",
      "Captain: What is the current shield percentage?\n",
      "AI: I will connect to the systems and get the info by calling  get_shield_percentage\n",
      "The current shield percentage is 55%.\n",
      "Captain: What is the dilithium core temperature?\n",
      "AI: I will connect to the systems and get the info by calling  get_core_temperature\n",
      "The dilithium core temperature is 4500 degrees.\n",
      "\n",
      "[SCENARIO END]\n",
      "Captain: 'Accurate data saved the day! Reroute power to shields and vent heat from the core!'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"[SCENARIO START]\")\n",
    "print(\"Enterprise is under attack. The Captain queries the AI for status:\\n\")\n",
    "\n",
    "# Captain asks the AI for shield status and core temperature\n",
    "query1 = \"What is the current shield percentage?\"\n",
    "query2 = \"What is the dilithium core temperature?\"\n",
    "\n",
    "#Call the OpenAI API with no tooling\n",
    "print(f\"Captain: {query1}\")\n",
    "response1 = ask_openai(query1)\n",
    "print(f\"AI: {response1.choices[0].message.content}\")\n",
    "\n",
    "print(f\"\\nCaptain: {query2}\")\n",
    "response2 = ask_openai(query2)\n",
    "print(f\"AI: {response2.choices[0].message.content}\")\n",
    "\n",
    "# Captain realizes the data is incorrect and asks for a solution\n",
    "print(\"\\n[ALERT] First Officer: 'This data doesn't help the situation!'\")\n",
    "print(\"Captain: 'AI, how can we get accurate data?'\")\n",
    "\n",
    "solution_query = \"How can you fetch real-time data?\"\n",
    "solution_response = ask_openai(solution_query)\n",
    "print(f\"AI: {solution_response.choices[0].message.content}\")\n",
    "\n",
    "# Implement tooling\n",
    "print(\"\\n[TOOL ENABLED] Connecting AI to the Enterprise telemetry system...\\n\")\n",
    "\n",
    "# Captain re-asks the questions\n",
    "print(f\"Captain: {query1}\")\n",
    "\n",
    "# the call to the function that has te information of the tools available.\n",
    "# This example uses 2 calls to get the infomration for the shields and temperature, but it can be implemented to use multiple functions on a single API call. \n",
    "#This should be done to avoid multiple calls to the API, but is done like this for easy reading and simplicity.\n",
    "live_response1 = ask_openai_with_functions(query1)\n",
    "\n",
    "#gets the function that will be called\n",
    "if live_response1.choices[0].finish_reason==\"tool_calls\":\n",
    "    function = live_response1.choices[0].message.tool_calls[0].function\n",
    "    print(f\"AI: I will connect to the systems and get the info by calling  {function.name}\")\n",
    "\n",
    "#call the local function to get the data\n",
    "eval_function = eval(function.name)\n",
    "content = eval_function()\n",
    "\n",
    "#send back the prompt with the function and the data and get the final response\n",
    "final_response_1  = ask_openai_complete_prompt(query1, function, content)\n",
    "\n",
    "\n",
    "print(final_response_1.choices[0].message.content)\n",
    "\n",
    "print(f\"Captain: {query2}\")\n",
    "live_response2 = ask_openai_with_functions(query2)\n",
    "\n",
    "\n",
    "if live_response2.choices[0].finish_reason==\"tool_calls\":\n",
    "    function = live_response2.choices[0].message.tool_calls[0].function\n",
    "    print(f\"AI: I will connect to the systems and get the info by calling  {function.name}\")\n",
    "\n",
    "eval_function = eval(function.name)\n",
    "content = eval_function()\n",
    "\n",
    "final_response_2  = ask_openai_complete_prompt(query2, function, content)\n",
    "\n",
    "\n",
    "print(final_response_2.choices[0].message.content)\n",
    "\n",
    "\n",
    "# Victory\n",
    "print(\"\\n[SCENARIO END]\")\n",
    "print(\"Captain: 'Accurate data saved the day! Reroute power to shields and vent heat from the core!'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bck_service",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
